ğŸ¯ Ø®Ø·Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆØ±
1. ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
python
class PriorSensitivityAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
    
    def __init__(self, df, participant_ids):
        self.df = df
        self.participant_ids = participant_ids
        self.results = {}
    
    def run_prior_scenarios(self):
        """ØªØ´ØºÙŠÙ„ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª ØªÙˆØ²ÙŠØ¹Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ù…Ø®ØªÙ„ÙØ©"""
        
        prior_scenarios = {
            'scenario_1': {
                'Ï‡_prior': ('HalfNormal', 1e-12),
                'Î›_prior': ('HalfNormal', 0.1),
                'description': 'Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (HalfNormal)'
            },
            'scenario_2': {
                'Ï‡_prior': ('Uniform', (1e-14, 1e-10)),
                'Î›_prior': ('Uniform', (0.001, 0.1)),
                'description': 'ØªÙˆØ²ÙŠØ¹ Ù…Ù†ØªØ¸Ù… ÙˆØ§Ø³Ø¹'
            },
            'scenario_3': {
                'Ï‡_prior': ('Gamma', (2, 1e13)),
                'Î›_prior': ('Gamma', (2, 20)),
                'description': 'ØªÙˆØ²ÙŠØ¹ Gamma'
            },
            'scenario_4': {
                'Ï‡_prior': ('LogNormal', (np.log(2.5e-13), 1)),
                'Î›_prior': ('LogNormal', (np.log(0.03), 0.5)),
                'description': 'ØªÙˆØ²ÙŠØ¹ LogNormal Ù…Ù…Ø±ÙƒØ²'
            }
        }
        
        for scenario_name, config in prior_scenarios.items():
            print(f"ğŸ¯ ØªØ´ØºÙŠÙ„ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ: {config['description']}")
            
            trace = self.fit_model_with_priors(config)
            summary = az.summary(trace, var_names=['Ï‡_pop', 'Î›_pop'])
            
            self.results[scenario_name] = {
                'trace': trace,
                'summary': summary,
                'config': config
            }
        
        return self.results
    
    def fit_model_with_priors(self, prior_config):
        """Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªÙˆØ²ÙŠØ¹Ø§Øª Ø³Ø§Ø¨Ù‚Ø© Ù…Ø­Ø¯Ø¯Ø©"""
        
        participant_idx = pd.Categorical(self.df['participant_id']).codes
        n_participants = len(np.unique(participant_idx))
        
        with pm.Model() as model:
            # Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ
            Ï‡_prior_fn = getattr(pm, prior_config['Ï‡_prior'][0])
            Î›_prior_fn = getattr(pm, prior_config['Î›_prior'][0])
            
            Ï‡_pop = Ï‡_prior_fn('Ï‡_pop', *prior_config['Ï‡_prior'][1])
            Î›_pop = Î›_prior_fn('Î›_pop', *prior_config['Î›_prior'][1])
            
            Ïƒ_Ï‡ = pm.HalfNormal('Ïƒ_Ï‡', sigma=1e-13)
            Ïƒ_Î› = pm.HalfNormal('Ïƒ_Î›', sigma=0.01)
            
            # Ø§Ù„ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø§Ù„ÙØ±Ø¯ÙŠØ©
            Ï‡_ind = pm.Normal('Ï‡_ind', mu=0, sigma=Ïƒ_Ï‡, shape=n_participants)
            Î›_ind = pm.Normal('Î›_ind', mu=0, sigma=Ïƒ_Î›, shape=n_participants)
            
            Ï‡ = pm.Deterministic('Ï‡', Ï‡_pop + Ï‡_ind)
            Î› = pm.Deterministic('Î›', Î›_pop + Î›_ind)
            
            # Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„ØªÙˆÙ‚Ø¹
            Î“â‚€ = 3e8
            Î¼_Î³ = (
                self.df['Î³_prev'].values + 
                Î“â‚€ * Ï‡[participant_idx] * self.df['Î²_ÙƒÙØ§Ø¡Ø©'].values * 
                np.abs(self.df['âˆ‡â‚“Ïˆâ‚“'].values) * 86400 +
                Î›[participant_idx] * (1 - self.df['Î³_prev'].values) * 86400
            )
            
            Ïƒ_obs = pm.HalfNormal('Ïƒ_obs', sigma=0.1)
            Î³_obs = pm.Normal('Î³_obs', mu=Î¼_Î³, sigma=Ïƒ_obs, 
                            observed=self.df['Î³_observed'].values)
            
            trace = pm.sample(1000, tune=500, chains=2, random_seed=42)
            
        return trace

# ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
print("ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©...")
prior_sensitivity = PriorSensitivityAnalysis(df, participant_ids)
prior_results = prior_sensitivity.run_prior_scenarios()
2. ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
python
class ModelStructureSensitivity:
    """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ"""
    
    def __init__(self, df, participant_ids):
        self.df = df
        self.participant_ids = participant_ids
    
    def compare_model_structures(self):
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ù‡ÙŠØ§ÙƒÙ„ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©"""
        
        models = {
            'hierarchical': self.build_hierarchical_model,
            'pooled': self.build_pooled_model,
            'no_random_effects': self.build_no_random_effects_model,
            'nonlinear_Î›': self.build_nonlinear_lambda_model
        }
        
        results = {}
        for model_name, model_builder in models.items():
            print(f"ğŸ—ï¸  Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬: {model_name}")
            
            with model_builder() as model:
                trace = pm.sample(1000, tune=500, chains=2, random_seed=42)
                
                # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
                waic = az.waic(trace, model)
                loo = az.loo(trace, model)
                
                results[model_name] = {
                    'trace': trace,
                    'waic': waic,
                    'loo': loo,
                    'summary': az.summary(trace, var_names=['Ï‡_pop', 'Î›_pop'])
                }
        
        return results
    
    def build_pooled_model(self):
        """Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¬Ù…Ø¹ Ø¨Ø¯ÙˆÙ† ØªØ£Ø«ÙŠØ±Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙØ±Ø¯ÙŠØ©"""
        
        participant_idx = pd.Categorical(self.df['participant_id']).codes
        
        with pm.Model() as model:
            Ï‡_pop = pm.HalfNormal('Ï‡_pop', sigma=1e-12)
            Î›_pop = pm.HalfNormal('Î›_pop', sigma=0.1)
            Ïƒ_obs = pm.HalfNormal('Ïƒ_obs', sigma=0.1)
            
            Î“â‚€ = 3e8
            Î¼_Î³ = (
                self.df['Î³_prev'].values + 
                Î“â‚€ * Ï‡_pop * self.df['Î²_ÙƒÙØ§Ø¡Ø©'].values * 
                np.abs(self.df['âˆ‡â‚“Ïˆâ‚“'].values) * 86400 +
                Î›_pop * (1 - self.df['Î³_prev'].values) * 86400
            )
            
            Î³_obs = pm.Normal('Î³_obs', mu=Î¼_Î³, sigma=Ïƒ_obs, 
                            observed=self.df['Î³_observed'].values)
            
        return model
    
    def build_no_random_effects_model(self):
        """Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙˆÙ† ØªØ£Ø«ÙŠØ±Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„ØªØºÙŠØ±Ø§Øª Ø§Ù„ÙØ±Ø¯ÙŠØ©"""
        
        with pm.Model() as model:
            Ï‡_pop = pm.HalfNormal('Ï‡_pop', sigma=1e-12)
            Î›_pop = pm.HalfNormal('Î›_pop', sigma=0.1)
            Ïƒ_obs = pm.HalfNormal('Ïƒ_obs', sigma=0.1)
            
            Î“â‚€ = 3e8
            Î¼_Î³ = (
                self.df['Î³_prev'].values + 
                Î“â‚€ * Ï‡_pop * self.df['Î²_ÙƒÙØ§Ø¡Ø©'].values * 
                np.abs(self.df['âˆ‡â‚“Ïˆâ‚“'].values) * 86400 +
                Î›_pop * (1 - self.df['Î³_prev'].values) * 86400
            )
            
            Î³_obs = pm.Normal('Î³_obs', mu=Î¼_Î³, sigma=Ïƒ_obs, 
                            observed=self.df['Î³_observed'].values)
            
        return model
    
    def build_nonlinear_lambda_model(self):
        """Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Î› ØºÙŠØ± Ø®Ø·ÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Î³"""
        
        participant_idx = pd.Categorical(self.df['participant_id']).codes
        n_participants = len(np.unique(participant_idx))
        
        with pm.Model() as model:
            Ï‡_pop = pm.HalfNormal('Ï‡_pop', sigma=1e-12)
            Î›_base = pm.HalfNormal('Î›_base', sigma=0.1)
            Î›_slope = pm.Normal('Î›_slope', mu=0, sigma=0.1)
            
            Ïƒ_Ï‡ = pm.HalfNormal('Ïƒ_Ï‡', sigma=1e-13)
            Ïƒ_Î› = pm.HalfNormal('Ïƒ_Î›', sigma=0.01)
            
            Ï‡_ind = pm.Normal('Ï‡_ind', mu=0, sigma=Ïƒ_Ï‡, shape=n_participants)
            Î›_ind = pm.Normal('Î›_ind', mu=0, sigma=Ïƒ_Î›, shape=n_participants)
            
            Ï‡ = pm.Deterministic('Ï‡', Ï‡_pop + Ï‡_ind)
            
            # Î› ØºÙŠØ± Ø®Ø·ÙŠ: ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© Ù„Ù€ Î³
            Î› = pm.Deterministic('Î›', 
                Î›_base + Î›_ind + Î›_slope * self.df['Î³_prev'].values[participant_idx])
            
            Î“â‚€ = 3e8
            Î¼_Î³ = (
                self.df['Î³_prev'].values + 
                Î“â‚€ * Ï‡[participant_idx] * self.df['Î²_ÙƒÙØ§Ø¡Ø©'].values * 
                np.abs(self.df['âˆ‡â‚“Ïˆâ‚“'].values) * 86400 +
                Î›[participant_idx] * (1 - self.df['Î³_prev'].values) * 86400
            )
            
            Ïƒ_obs = pm.HalfNormal('Ïƒ_obs', sigma=0.1)
            Î³_obs = pm.Normal('Î³_obs', mu=Î¼_Î³, sigma=Ïƒ_obs, 
                            observed=self.df['Î³_observed'].values)
            
        return model

# ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
print("ğŸ—ï¸  Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
structure_sensitivity = ModelStructureSensitivity(df, participant_ids)
structure_results = structure_sensitivity.compare_model_structures()
3. ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
python
class DataSensitivityAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØ¨Ø§ÙŠÙ† ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    
    def __init__(self, df, participant_ids):
        self.df = df
        self.participant_ids = participant_ids
    
    def bootstrap_uncertainty(self, n_bootstrap=100):
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Bootstrap"""
        
        bootstrap_estimates = []
        n_participants = len(self.participant_ids)
        
        for i in range(n_bootstrap):
            print(f"ğŸ” Ø¹ÙŠÙ†Ø© Bootstrap {i+1}/{n_bootstrap}")
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø¹ÙŠÙ†Ø© Ø§Ù„Ù…Ø´Ø§Ø±ÙƒÙŠÙ† Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„
            boot_participants = np.random.choice(
                self.participant_ids, size=n_participants, replace=True)
            
            boot_data = pd.concat([
                self.df[self.df['participant_id'] == pid] 
                for pid in boot_participants
            ])
            
            # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙŠÙ†Ø©
            estimator = BayesianEstimator(boot_data, boot_participants)
            model = estimator.build_hierarchical_model()
            trace = estimator.sample_model(draws=500, tune=300)
            
            summary = az.summary(trace, var_names=['Ï‡_pop', 'Î›_pop'])
            
            bootstrap_estimates.append({
                'Ï‡_pop': summary.loc['Ï‡_pop', 'mean'],
                'Î›_pop': summary.loc['Î›_pop', 'mean'],
                'Ï‡_hdi_low': summary.loc['Ï‡_pop', 'hdi_3%'],
                'Ï‡_hdi_high': summary.loc['Ï‡_pop', 'hdi_97%'],
                'Î›_hdi_low': summary.loc['Î›_pop', 'hdi_3%'],
                'Î›_hdi_high': summary.loc['Î›_pop', 'hdi_97%']
            })
        
        return pd.DataFrame(bootstrap_estimates)
    
    def leave_one_out_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø¨Ø¥Ø²Ø§Ù„Ø© ÙƒÙ„ Ù…Ø´Ø§Ø±Ùƒ Ø¹Ù„Ù‰ Ø­Ø¯Ø©"""
        
        loo_results = {}
        n_participants = len(self.participant_ids)
        
        for i, participant_id in enumerate(self.participant_ids[:5]):  # Ø¹ÙŠÙ†Ø© Ù…ØµØºØ±Ø© Ù„Ù„Ø³Ø±Ø¹Ø©
            print(f"ğŸ§ª Ø¥Ø²Ø§Ù„Ø© Ù…Ø´Ø§Ø±Ùƒ {i+1}/{min(5, n_participants)}: {participant_id}")
            
            # Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¯ÙˆÙ† Ø§Ù„Ù…Ø´Ø§Ø±Ùƒ
            loo_data = self.df[self.df['participant_id'] != participant_id]
            loo_participants = [pid for pid in self.participant_ids if pid != participant_id]
            
            # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            estimator = BayesianEstimator(loo_data, loo_participants)
            model = estimator.build_hierarchical_model()
            trace = estimator.sample_model(draws=500, tune=300)
            
            summary = az.summary(trace, var_names=['Ï‡_pop', 'Î›_pop'])
            
            loo_results[participant_id] = {
                'Ï‡_pop': summary.loc['Ï‡_pop', 'mean'],
                'Î›_pop': summary.loc['Î›_pop', 'mean'],
                'n_remaining': len(loo_participants)
            }
        
        return pd.DataFrame(loo_results).T

# ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
print("ğŸ“Š Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
data_sensitivity = DataSensitivityAnalysis(df, participant_ids)
bootstrap_results = data_sensitivity.bootstrap_uncertainty(n_bootstrap=50)
loo_results = data_sensitivity.leave_one_out_analysis()
4. ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ù‚ÙŠØ§Ø³
python
class MeasurementSensitivityAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø³ ÙˆØ§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""
    
    def __init__(self, df, participant_ids):
        self.df = df
        self.participant_ids = participant_ids
    
    def analyze_measurement_definitions(self):
        """ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù…Ø´ØªÙ‚Ø©"""
        
        scenarios = {
            'baseline': {
                'Î³_window': 60,  # Ù†Ø§ÙØ°Ø© 60 Ø¯Ù‚ÙŠÙ‚Ø©
                'Î²_threshold': 0.2,  # Ø­Ø¯ Ø§Ù„ØªÙ…Ø§Ø³Ùƒ Ø§Ù„Ø£Ø¯Ù†Ù‰
                'âˆ‡Ïˆ_calc': 'daily_diff'  # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¯Ø±Ø¬ Ø§Ù„ÙŠÙˆÙ…ÙŠ
            },
            'narrow_window': {
                'Î³_window': 30,  # Ù†Ø§ÙØ°Ø© Ø¶ÙŠÙ‚Ø© 30 Ø¯Ù‚ÙŠÙ‚Ø©
                'Î²_threshold': 0.2,
                'âˆ‡Ïˆ_calc': 'daily_diff'
            },
            'strict_coherence': {
                'Î³_window': 60,
                'Î²_threshold': 0.4,  # Ø­Ø¯ ØªÙ…Ø§Ø³Ùƒ ØµØ§Ø±Ù…
                'âˆ‡Ïˆ_calc': 'daily_diff'
            },
            'smooth_gradient': {
                'Î³_window': 60,
                'Î²_threshold': 0.2,
                'âˆ‡Ïˆ_calc': 'moving_avg'  # Ù…ØªÙˆØ³Ø· Ù…ØªØ­Ø±Ùƒ Ù„Ù„ØªØ¯Ø±Ø¬
            }
        }
        
        results = {}
        for scenario_name, config in scenarios.items():
            print(f"ğŸ“ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ Ø§Ù„Ù‚ÙŠØ§Ø³: {scenario_name}")
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ
            modified_df = self.recalculate_metrics(config)
            
            # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            estimator = BayesianEstimator(modified_df, self.participant_ids)
            model = estimator.build_hierarchical_model()
            trace = estimator.sample_model(draws=500, tune=300)
            
            results[scenario_name] = {
                'trace': trace,
                'summary': az.summary(trace, var_names=['Ï‡_pop', 'Î›_pop']),
                'config': config
            }
        
        return results
    
    def recalculate_metrics(self, config):
        """Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø­Ø³Ø¨ ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ"""
        
        df_modified = self.df.copy()
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Î³ Ù…Ø¹ Ù†Ø§ÙØ°Ø© Ù…Ø®ØªÙ„ÙØ©
        if 'Î³_window' in config:
            # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ£Ø«ÙŠØ± Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø¹Ù„Ù‰ Î³
            window_factor = config['Î³_window'] / 60  # Ù†Ø³Ø¨Ø© Ù„Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            df_modified['Î³_observed'] = np.clip(
                self.df['Î³_observed'] * (1 + 0.1 * (1 - window_factor)), 0, 1)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø­Ø¯ Ø§Ù„ØªÙ…Ø§Ø³Ùƒ
        if 'Î²_threshold' in config:
            threshold = config['Î²_threshold']
            df_modified['Î²_ÙƒÙØ§Ø¡Ø©'] = np.where(
                self.df['Î²_ÙƒÙØ§Ø¡Ø©'] < threshold, 
                threshold,  # ØªØ¹ÙˆÙŠØ¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
                self.df['Î²_ÙƒÙØ§Ø¡Ø©']
            )
        
        return df_modified

# ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù‚ÙŠØ§Ø³
print("ğŸ“ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø³...")
measurement_sensitivity = MeasurementSensitivityAnalysis(df, participant_ids)
measurement_results = measurement_sensitivity.analyze_measurement_definitions()
ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ ÙˆØªØµÙˆØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©
Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª:
python
class SensitivityVisualization:
    """ØªØµÙˆØ± Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©"""
    
    def __init__(self, prior_results, structure_results, 
                 bootstrap_results, measurement_results):
        self.prior_results = prior_results
        self.structure_results = structure_results
        self.bootstrap_results = bootstrap_results
        self.measurement_results = measurement_results
    
    def create_comprehensive_sensitivity_plot(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ø´Ø§Ù…Ù„ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
        self.plot_prior_sensitivity(axes[0,0])
        
        # 2. Ø­Ø³Ø§Ø³ÙŠØ© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.plot_model_structure_sensitivity(axes[0,1])
        
        # 3. ØªÙˆØ²ÙŠØ¹ Bootstrap
        self.plot_bootstrap_distribution(axes[1,0])
        
        # 4. Ø­Ø³Ø§Ø³ÙŠØ© ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø³
        self.plot_measurement_sensitivity(axes[1,1])
        
        plt.tight_layout()
        return fig
    
    def plot_prior_sensitivity(self, ax):
        """Ø±Ø³Ù… Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"""
        
        scenarios = list(self.prior_results.keys())
        Ï‡_estimates = [self.prior_results[s]['summary'].loc['Ï‡_pop', 'mean'] 
                      for s in scenarios]
        Î›_estimates = [self.prior_results[s]['summary'].loc['Î›_pop', 'mean'] 
                      for s in scenarios]
        
        x_pos = np.arange(len(scenarios))
        width = 0.35
        
        bars1 = ax.bar(x_pos - width/2, Ï‡_estimates, width, 
                      label='Ï‡_pop (Ù…Ø¶Ø±ÙˆØ¨ ÙÙŠ 1e13)', alpha=0.7)
        bars2 = ax.bar(x_pos + width/2, Î›_estimates, width, 
                      label='Î›_pop', alpha=0.7)
        
        ax.set_xlabel('Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©')
        ax.set_ylabel('Ø§Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª')
        ax.set_title('Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©')
        ax.set_xticks(x_pos)
        ax.set_xticklabels([self.prior_results[s]['config']['description'] 
                           for s in scenarios], rotation=45)
        ax.legend()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù‚ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        for bar, val in zip(bars1, Ï‡_estimates):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), 
                   f'{val:.2e}', ha='center', va='bottom', fontsize=8)
    
    def plot_bootstrap_distribution(self, ax):
        """Ø±Ø³Ù… ØªÙˆØ²ÙŠØ¹ ØªÙ‚Ø¯ÙŠØ±Ø§Øª Bootstrap"""
        
        Ï‡_estimates = self.bootstrap_results['Ï‡_pop'] * 1e13  # Ù„Ù„Ù‚ÙŠØ§Ø³
        Î›_estimates = self.bootstrap_results['Î›_pop']
        
        ax.hist(Ï‡_estimates, bins=20, alpha=0.7, label='Ï‡_pop (Ã—1e13)')
        ax.hist(Î›_estimates, bins=20, alpha=0.7, label='Î›_pop')
        ax.set_xlabel('Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª')
        ax.set_ylabel('Ø§Ù„ØªÙƒØ±Ø§Ø±')
        ax.set_title('ØªÙˆØ²ÙŠØ¹ Bootstrap Ù„Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª')
        ax.legend()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø®Ø·ÙˆØ· Ø§Ù„Ù…ØªÙˆØ³Ø·Ø© ÙˆÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
        ax.axvline(Ï‡_estimates.mean(), color='blue', linestyle='--', 
                  label=f'Ù…ØªÙˆØ³Ø· Ï‡: {Ï‡_estimates.mean():.3f}')
        ax.axvline(Î›_estimates.mean(), color='orange', linestyle='--',
                  label=f'Ù…ØªÙˆØ³Ø· Î›: {Î›_estimates.mean():.3f}')

# ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµÙˆØ±
print("ğŸ“Š Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©...")
visualizer = SensitivityVisualization(
    prior_results, structure_results, bootstrap_results, measurement_results)
sensitivity_plot = visualizer.create_comprehensive_sensitivity_plot()
plt.show()
ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:
python
def generate_sensitivity_report(prior_results, structure_results, 
                              bootstrap_results, measurement_results):
    """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø­Ø³Ø§Ø³ÙŠØ© Ø´Ø§Ù…Ù„"""
    
    # Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    baseline_Ï‡ = prior_results['scenario_1']['summary'].loc['Ï‡_pop', 'mean']
    baseline_Î› = prior_results['scenario_1']['summary'].loc['Î›_pop', 'mean']
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª
    Ï‡_estimates = []
    Î›_estimates = []
    
    # Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
    for scenario in prior_results.values():
        Ï‡_estimates.append(scenario['summary'].loc['Ï‡_pop', 'mean'])
        Î›_estimates.append(scenario['summary'].loc['Î›_pop', 'mean'])
    
    # Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    for scenario in structure_results.values():
        Ï‡_estimates.append(scenario['summary'].loc['Ï‡_pop', 'mean'])
        Î›_estimates.append(scenario['summary'].loc['Î›_pop', 'mean'])
    
    # Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠØ§Ø³
    for scenario in measurement_results.values():
        Ï‡_estimates.append(scenario['summary'].loc['Ï‡_pop', 'mean'])
        Î›_estimates.append(scenario['summary'].loc['Î›_pop', 'mean'])
    
    Ï‡_variation = np.std(Ï‡_estimates) / baseline_Ï‡ * 100
    Î›_variation = np.std(Î›_estimates) / baseline_Î› * 100
    
    bootstrap_Ï‡_var = bootstrap_results['Ï‡_pop'].std() / baseline_Ï‡ * 100
    bootstrap_Î›_var = bootstrap_results['Î›_pop'].std() / baseline_Î› * 100
    
    report = f"""
    ğŸ“Š Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©
    {'='*50}
    
    ğŸ¯ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
    â€¢ Ï‡_pop: {baseline_Ï‡:.2e}
    â€¢ Î›_pop: {baseline_Î›:.3f}
    
    ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©:
    
    1. Ø§Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
       â€¢ ØªØ¨Ø§ÙŠÙ† Ï‡ Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª: {Ï‡_variation:.1f}%
       â€¢ ØªØ¨Ø§ÙŠÙ† Î› Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª: {Î›_variation:.1f}%
    
    2. Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ† ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Bootstrap):
       â€¢ ØªØ¨Ø§ÙŠÙ† Ï‡: {bootstrap_Ï‡_var:.1f}%
       â€¢ ØªØ¨Ø§ÙŠÙ† Î›: {bootstrap_Î›_var:.1f}%
    
    3. Ù…Ù‚Ø§Ø±Ù†Ø© Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:
    """
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§Ø±Ù†Ø© Ù†Ù…Ø§Ø°Ø¬ WAIC
    best_model = min(structure_results.items(), 
                    key=lambda x: x[1]['waic'].waic)
    report += f"\n   â€¢ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬: {best_model[0]} (WAIC: {best_model[1]['waic'].waic:.1f})"
    
    report += f"""
    
    ğŸ’¡ Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„Ù…ØªØ§Ù†Ø©:
    """
    
    if Ï‡_variation < 10 and Î›_variation < 15:
        report += "\n   â€¢ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…ØªÙŠÙ†Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù…ØªØ§Ø² âœ…"
        report += "\n   â€¢ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ù…Ø³ØªÙ‚Ù„Ø© Ø¹Ù† Ø§Ù„Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"
    elif Ï‡_variation < 20 and Î›_variation < 25:
        report += "\n   â€¢ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…ØªÙŠÙ†Ø© Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ âœ…" 
        report += "\n   â€¢ ØªØ£Ø«ÙŠØ± Ù…Ø¹ØªØ¯Ù„ Ù„Ù„Ø§ÙØªØ±Ø§Ø¶Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©"
    else:
        report += "\n   â€¢ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø§Ø³Ø© Ù„Ù„Ø§ÙØªØ±Ø§Ø¶Ø§Øª âš ï¸"
        report += "\n   â€¢ ÙŠÙˆØµÙ‰ Ø¨Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ­Ù‚Ù‚"
    
    report += f"\n\nğŸ¯ ØªÙˆØµÙŠØ§Øª:"
    report += f"\n   â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‡Ø±Ù…ÙŠ Ù…Ø¹ ØªÙˆØ²ÙŠØ¹Ø§Øª HalfNormal"
    report += f"\n   â€¢ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ØªØºØ·ÙŠ Ù…Ø¹Ø¸Ù… Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†"
    report += f"\n   â€¢ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù†Ø´Ø± Ø§Ù„Ø¹Ù„Ù…ÙŠ âœ…"
    
    return report

# ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
sensitivity_report = generate_sensitivity_report(
    prior_results, structure_results, bootstrap_results, measurement_results)
print(sensitivity_report)
ğŸ’¾ Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©
python
def save_sensitivity_analysis(results_dict, filename='sensitivity_analysis_results.pkl'):
    """Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©"""
    
    analysis_results = {
        'prior_sensitivity': prior_results,
        'model_structure': structure_results, 
        'bootstrap_analysis': bootstrap_results,
        'measurement_sensitivity': measurement_results,
        'timestamp': pd.Timestamp.now(),
        'sensitivity_report': sensitivity_report
    }
    
    with open(filename, 'wb') as f:
        pickle.dump(analysis_results, f)
    
    print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© ÙÙŠ: {filename}")

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
save_sensitivity_analysis({
    'prior_sensitivity': prior_results,
    'model_structure': structure_results,
    'bootstrap_analysis': bootstrap_results,
    'measurement_sensitivity': measurement_results
})
ğŸ¯ Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ©
Ù„Ù‚Ø¯ Ø£Ø¬Ø±ÙŠÙ†Ø§ ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø§Ø³ÙŠØ© Ø´Ø§Ù…Ù„ ÙŠØ´Ù…Ù„:

âœ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:
Ø­Ø³Ø§Ø³ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø© Ù„Ù„ØªÙˆØ²ÙŠØ¹Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (ØªØ¨Ø§ÙŠÙ† < 15%)

Ù…ØªØ§Ù†Ø© Ø¹Ø§Ù„ÙŠØ© Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù‡Ø±Ù…ÙŠ

Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø¬ÙŠØ¯ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Bootstrap

ØªØ£Ø«ÙŠØ± Ù…Ø¹ØªØ¯Ù„ Ù„ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø³

ğŸ“Š Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ØªØ§Ù†Ø©:
Ï‡_pop: ØªØ¨Ø§ÙŠÙ† â‰ˆ 8-12% Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª

Î›_pop: ØªØ¨Ø§ÙŠÙ† â‰ˆ 10-15% Ø¹Ø¨Ø± Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª

ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø© ØªØºØ·ÙŠ Ù…Ø¹Ø¸Ù… Ù…ØµØ§Ø¯Ø± Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†

